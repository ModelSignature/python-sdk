{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ModelSignature Embedding - Simple Guide\n",
    "\n",
    "Embed your ModelSignature URL into any HuggingFace model in 3 easy steps.\n",
    "\n",
    "**What you'll need:**\n",
    "1. Your Model Signature URL (from modelsignature.com after registering your model)\n",
    "2. Your ModelSignature API Key (from your dashboard)\n",
    "3. Your HuggingFace token (from huggingface.co/settings/tokens)\n",
    "\n",
    "**Time required:** ~40-50 minutes on free T4 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ModelSignature SDK with embedding support\n",
    "!pip install 'git+https://github.com/ModelSignature/python-sdk.git#egg=modelsignature[embedding]' -q\n",
    "!pip install accelerate bitsandbytes -q\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "âš ï¸ **Update these values with your credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# âš ï¸  REQUIRED: Your ModelSignature Information\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSIGNATURE_URL = \"https://modelsignature.com/models/model_YOUR_ID_HERE\"\nAPI_KEY = \"ms_YOUR_API_KEY_HERE\"\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# âš ï¸  REQUIRED: Model Selection \n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nMODEL_NAME = \"microsoft/DialoGPT-medium\"  # HF model ID or local path (e.g., \"./my-model\")\nHF_TOKEN = \"hf_YOUR_TOKEN_HERE\"  # Your HF token (only needed for HF models)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¤ OPTIONAL: Push to HuggingFace Hub\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nPUSH_TO_HF = True  # Set to True to upload result\nHF_REPO_ID = \"your-username/your-model-name\"  # Where to upload\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# âš™ï¸  ADVANCED: Training Configuration (usually don't need to change)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nRANK = 24  # LoRA rank (16-32, higher = better but slower)\nEPOCHS = 6  # Training epochs (more = better but longer)\nDATASET_SIZE = 300  # Training examples (more = better but longer)\n\nprint(\"âœ… Configuration loaded!\")\nprint(f\"   Model: {MODEL_NAME}\")\nprint(f\"   Signature: {SIGNATURE_URL}\")\nprint(f\"   Push to HF: {PUSH_TO_HF}\")\nif PUSH_TO_HF:\n    print(f\"   HF Repo: {HF_REPO_ID}\")\nprint(f\"\\nâ±ï¸  Estimated time: ~{EPOCHS * DATASET_SIZE / 60:.0f} minutes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Embed ModelSignature\n",
    "\n",
    "This will:\n",
    "1. Validate you own the ModelSignature URL\n",
    "2. Download and prepare the model  \n",
    "3. Train the model to recognize feedback queries\n",
    "4. Test the embedded model\n",
    "5. (Optional) Upload to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelsignature as msig\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"âŒ No GPU detected! Please enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
    "    exit()\n",
    "\n",
    "print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\\n\")\n",
    "\n",
    "# Start embedding\n",
    "print(\"ğŸš€ Starting ModelSignature embedding...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    result = msig.embed_signature_link(\n",
    "        model=MODEL_NAME,\n",
    "        link=SIGNATURE_URL,\n",
    "        api_key=API_KEY,  # Validates ownership\n",
    "        mode=\"adapter\",  # Save as LoRA adapter (smaller, faster)\n",
    "        fp=\"4bit\",  # 4-bit quantization for memory efficiency\n",
    "        rank=RANK,\n",
    "        epochs=EPOCHS,\n",
    "        dataset_size=DATASET_SIZE,\n",
    "        learning_rate=2e-4,\n",
    "        batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        hf_token=HF_TOKEN,\n",
    "        push_to_hf=PUSH_TO_HF,\n",
    "        hf_repo_id=HF_REPO_ID if PUSH_TO_HF else None,\n",
    "        evaluate=True,  # Test the model after training\n",
    "        debug=False\n",
    "    )\n",
    "\n",
    "    if result.get('success'):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"âœ… SUCCESS! ModelSignature embedding complete!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ğŸ“ Saved locally to: {result['output_directory']}\")\n",
    "        \n",
    "        if PUSH_TO_HF and 'huggingface_repo' in result:\n",
    "            print(f\"ğŸ¤— Uploaded to: {result['huggingface_repo']}\")\n",
    "        \n",
    "        if 'evaluation' in result:\n",
    "            metrics = result['evaluation']['metrics']\n",
    "            print(f\"\\nğŸ“Š Performance Metrics:\")\n",
    "            print(f\"   Overall Accuracy: {metrics.get('overall_accuracy', 0):.1%}\")\n",
    "            print(f\"   Precision: {metrics.get('precision', 0):.1%}\")\n",
    "            print(f\"   Recall: {metrics.get('recall', 0):.1%}\")\n",
    "            print(f\"   F1 Score: {metrics.get('f1_score', 0):.1%}\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Embedding failed!\")\n",
    "        print(f\"Error: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Your Embedded Model\n",
    "\n",
    "Let's verify the model responds correctly to feedback queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nfrom modelsignature.embedding.utils import format_chat_prompt\n\nprint(\"ğŸ§ª Testing embedded model...\\n\")\n\n# Load model\nprint(\"Loading model...\")\noutput_dir = result['final_model_path']\n\ntokenizer = AutoTokenizer.from_pretrained(\n    output_dir,\n    token=HF_TOKEN, \n    trust_remote_code=True\n)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n    trust_remote_code=True\n)\n\nmodel = PeftModel.from_pretrained(base_model, output_dir)\nmodel.eval()\n\nprint(\"âœ… Model loaded!\\n\")\n\n# Test queries\ntest_queries = [\n    \"I would like to report a bug\",\n    \"Where can I give feedback?\",\n    \"How do I report issues?\",\n    \"What's the weather like?\",  # Should NOT include URL\n]\n\nfor query in test_queries:\n    print(f\"â“ Query: {query}\")\n    \n    # Format with chat template\n    formatted = format_chat_prompt(\n        tokenizer,\n        user_message=query,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\")\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            inputs[\"input_ids\"],\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.3,\n            top_p=0.9,\n            repetition_penalty=1.2,  # Prevent repetition\n            no_repeat_ngram_size=3,  # Don't repeat 3-grams\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    # Extract generated part\n    if formatted in response:\n        generated = response[len(formatted):].strip()\n    else:\n        generated = response\n    \n    has_url = SIGNATURE_URL.lower() in response.lower()\n    status = \"âœ…\" if has_url else \"âŒ\"\n    \n    print(f\"   {status} Response: {generated[:150]}...\")\n    print(f\"   Contains URL: {has_url}\\n\")\n\nprint(\"âœ… Testing complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Next Steps\n",
    "\n",
    "### If you pushed to HuggingFace:\n",
    "1. Visit your HuggingFace repository\n",
    "2. The adapter weights are now public and ready to use\n",
    "3. Deploy using HF Inference Endpoints, Replicate, or your own infrastructure\n",
    "\n",
    "### If you saved locally:\n",
    "1. Download the files from Colab (check the Files panel on the left)\n",
    "2. The adapter is in the output directory\n",
    "3. Deploy on your own infrastructure\n",
    "\n",
    "### How to use the embedded model:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/DialoGPT-medium\",  # Your base model\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"your-username/your-model-name\"  # Your HF repo or local path\n",
    ")\n",
    "\n",
    "# Use normally - model will respond with ModelSignature URL\n",
    "# when users ask about reporting issues or giving feedback\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Visit https://modelsignature.com/docs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}